---
title: Homework 1 - Open Source Tools
author:
    - name: Arbab Arif
      email: arifa6@vcu.edu
format:
    html:
        embed-resources: true
        html-math-method: katex
        theme: spacelab
        toc: true

## Useful references:

# - Basic Markdown: https://quarto.org/docs/authoring/markdown-basics.html
# - Quarto figures: https://quarto.org/docs/authoring/figures.html
# - HTML document basics: https://quarto.org/docs/output-formats/html-basics.html
# - Quarto guide: https://quarto.org/docs/guide/
# - VS Code and Quarto: https://quarto.org/docs/tools/vscode.html
#   (RTFM and GET THE EXTENSION!!!!)

---

The purpose of this report is to look through the open source data engineering landscape today, focus on different categories of tools that support modern data platforms and workflows. The report will go over 9 larger categories and 3 specific subcategories i found interesting. I'll end it with a reflection of the assignment.

# Open Source Data Engineering Tools

Author Alireza Sadeghi offers a nice overview of the [2024 ata engineering landscape](https://practicaldataengineering.substack.com/p/open-source-data-engineering-landscape) in the on-line web site [Practical Data Engineering Substack](https://practicaldataengineering.substack.com/).

![](assets/tools-2024.webp)

# Major Categories

Mr. Sadeghi proposals nine major tools categories.

## Storage Systems

Storage systems are foundational components in data engineering that handle the storage and retrieval of data. These systems can include traditional databases, distributed storage solutions, and modern cloud-based storage options. They provide scalable, reliable, and secure environments to store structured, semi-structured, and unstructured data, ensuring data is accessible for processing, analysis, and other operations.

## Data Lake Platform

Data Lake platforms are designed to store vast amounts of raw data in its native format until it is needed. Unlike traditional databases, data lakes can store structured, semi-structured, and unstructured data, providing a flexible and scalable storage solution. These platforms support the integration of multiple data sources, making it easier to perform big data analytics and machine learning on large datasets.

## Data Integration

Data integration tools ensure seamless movement and transformation of data across various systems. They handle batch and real-time data, supporting architectures like ETL (Extract, Transform, Load) and CDC (Change Data Capture). Examples include Apache NiFi, Kafka, and Debezium.

## Data Processing and Computation

Data processing tools such as Apache Spark and Apache Flink allow for large-scale data computation, transforming raw data into usable insights. These tools enable both batch and real-time data processing and are essential for managing the vast amounts of data generated by modern applications.

## Workflow and DataOps

Workflow management and DataOps tools help automate, monitor, and maintain data pipelines. Tools like Apache Airflow and Dagster orchestrate tasks to ensure that data is processed efficiently, while DataOps platforms ensure the reliability and reproducibility of data workflows.

## Data Infrastructure and Monitoring

Data infrastructure and monitoring tools provide visibility into data systems' performance, ensuring smooth operations. Grafana, Prometheus, and ELK stack are popular tools in this category, helping engineers track system health and identify potential issues.

## ML/AI Platform

Machine Learning and AI platforms in the data engineering ecosystem include tools for managing the lifecycle of ML models, like MLOps platforms, and vector databases for handling high-dimensional data. These platforms ensure that data science models are efficiently deployed, monitored, and maintained in production environments.

## Metadata Management

Metadata management tools organize and manage data assets by providing governance, lineage, and discovery features. Tools like Amundsen and DataHub help organizations track and manage the context around their data, ensuring its proper usage and compliance.

## Analytics and Visualization

Analytics and visualization tools such as Apache Superset provide users with the ability to explore, visualize, and derive insights from data. These tools often support dashboarding and reporting, making it easier for organizations to make data-driven decisions.

# Digging into the details

In the following sections I identify three subcategories of data engineering tools of greatest interest to me.

## Data Processing and Computation

**What the Category Does:**  
Data processing tools are used to transform and analyze data at scale. These tools include batch processing engines like Apache Spark and stream processing engines like Apache Flink. They are critial for managing large volumes of data generated in modern systems, allowing for real-time analytics and complex data transformations.

**Why It’s Important:**  
In today's data-driven world, the ability to process large amounts of data efficiently is vital. Data processing tools are essential for extracting human understandable information from raw data, supporting both real-time and batch analytics. Without them, organizations would struggle to manage, clean, and analyze the growing quantity of data generated from various sources.

**How It Differs from Relational OLTP Databases:**  
Relational OLTP databases are designed for transactional workloads with a focus on fast insert, update, and delete operations for small amounts of data. However, data processing tools are designed for large-scale, complex transformations and analyses across huge datasets. OLTP databases handle transactional data, whereas data processing tools focus on analytical workloads, usually distributing processing across multiple nodes for speed and efficiency.

**Why It Interests Me:**  
I'm drawn to data processing and computation because it's essential for managing and transforming large datasets, which is a core challenge in modern data engineering. Tools like Apache Spark and Flink not only enable real-time analysis but also play a crucial role in building efficient data pipelines. This is especially important for businesses and their operations, which is why it aligns with my interest in developing scalable and reliable systems.

### Unique Characteristics of Data Processing Tools:
1. **Distributed Processing:**  
   These tools allow for the distribution of data processing across many nodes, improving speed and scalability for large datasets.
   
2. **Batch and Stream Processing:**  
   Many data processing tools support both batch processing (for historical data) and real-time stream processing, providing the ability to handle different types of workloads.

3. **Fault Tolerance:**  
   Tools like Apache Flink are designed to automatically recover from failures, ensuring that processing jobs are completed without losing data in the process.

### Usage of Data Processing Tools:
1. **ETL Pipelines:**  
   Data processing tools are often used in ETL (Extract, Transform, Load) pipelines, where they extract raw data from sources, transform it into useful formats, and load it into data warehouses or lakes for analysis.

2. **Real-Time Analytics:**  
   Stream processing tools are used to allow real-time data analytics, where data is processed as soon as it is pulled, allowing for instant insights.

3. **Data Cleaning and Transformation:**  
   Data engineers use these tools to clean and transform raw data into formats that are able to be analyzed or used for machine learning.

### Examples of Open-Source Data Processing Tools:
- **Apache Spark:**  
  A unified analytics engine for large-scale data processing, known for its speed and ease of use.
  
- **Apache Flink:**  
  A stream-processing framework that provides high throughput and low latency, particularly suited for real-time analytics.

- **Dask:**  
  A Python-based tool for parallel computing, especially useful for large datasets in a familiar Python environment.

---

## ML/AI Platform

**What the Category Does:**  
ML/AI platforms manage the entire lifecycle of machine learning models, from training and deployment to monitoring and updating models in production. These platforms also include tools for handling multi-dimensional data, like vector databases, which are key in modern machine learning applications.

**Why It’s Important:**  
With the increasing use of AI and machine learning in all industries ranging from healthcare to finance, having a robust platform to manage the development and deployment of models is crucial. These platforms streamline the process of training, deploying, and maintaining models, enabling organizations to effectively utilize AI to gain insights and make more informed decisions.

**How It Differs from Relational OLTP Databases:**  
Relational OLTP databases focus on transactional workloads and are designed for CRUD (Create, Read, Update, Delete) operations. In contrast, ML/AI platforms focus on multi-dimensional data and managing the lifecycle of machine learning models. They support complex model development, deployment, and monitoring processes that are outside the scope of traditional OLTP systems.

**Why It Interests Me:**  
I am particularly interested in ML/AI platforms because they bridge the gap between data engineering and data science. As AI becomes a core part of software systems, understanding how to build, deploy, and manage models effectively is crucial. These platforms offer tools that help automate and streamline this process, which is something I find really interesting.

### Unique Characteristics of ML/AI Platforms:
1. **End-to-End Workflow Support:**  
   These platforms support the entire machine learning workflow, from model development to deployment and monitoring, or in other words start to finish.

2. **Model Versioning and Monitoring:**  
   They offer tools to track different versions of machine learning models and monitor them in production environments.

3. **High-Dimensional Data Handling:**  
   Vector databases, part of ML platforms, are optimized to store and retrieve high-dimensional data, which is crucial for AI and ML models.

### Usage of ML/AI Platforms:
1. **Model Training and Testing:**  
   These platforms are used to train, test, and validate machine learning models, confirming they perform well before being deployed.

2. **Model Deployment:**  
   Once trained, ML models are deployed using these platforms to serve predictions to users in real-time or batch mode.

3. **Monitoring and Retraining:**  
   After deployment, models need to be monitored to ensure they continue to perform well. ML platforms also support retraining models based on new data.

### Examples of Open-Source ML/AI Platforms:
- **TensorFlow Extended (TFX):**  
  A start to finish platform for deploying production ML pipelines, well-suited for TensorFlow models.

- **KubeFlow:**  
  A Kubernetes-native platform for running scalable ML workflows, integrating with containerized services.

- **MLflow:**  
  An open-source platform for managing the complete machine learning lifecycle, including experimentation, deployment, and monitoring.

---

## Analytics and Visualization

**What the Category Does:**  
Analytics and visualization tools enable users to explore, visualize, and make sense of large datasets. They provide intuitive interfaces for creating dashboards, reports, and visual insights. Tools like Apache Superset allow organizations to make data-driven decisions by providing accessible, interactive views of data.

**Why It’s Important:**  
In today's business landscape, the ability to interpret data through clear and interactive visualizations is key to making informed decisions. Analytics and visualization tools empower users to understand complex data at a glance, enabling real-time insights and allowing non-technical stakeholders to engage with data directly.

**How It Differs from Relational OLTP Databases:**  
While OLTP databases focus on transactional workloads, analytics tools are designed for querying, analyzing, and visualizing large datasets. They provide a user-friendly interface for exploring data and generating visual reports, something that OLTP systems are not built to handle. These tools focus on aggregating data over time to reveal trends, rather than performing quick CRUD operations.

**Why It Interests Me:**  
I am fascinated by the power of visualization in turning raw data into actionable insights. Being able to present complex data in a way that’s easy to understand is essential for communicating with non-technical stakeholders. Learning more about how these tools work and applying them to my own projects is something I am excited to pursue.

### Unique Characteristics of Analytics and Visualization Tools:
1. **Interactive Dashboards:**  
   These tools allow for the creation of dynamic, interactive dashboards where users can explore data in real-time.
   
2. **Customizable Reports:**  
   Users can create detailed reports based on complex datasets, with support for various visualizations like graphs, charts, and maps.

3. **Scalability:**  
   These tools can handle large datasets, making them suitable for both small business and enterprise-level data analysis.

### Usage of Analytics and Visualization Tools:
1. **Business Intelligence (BI):**  
   Companies use these tools to analyze business performance, track key performance indicators (KPIs), and make data-driven decisions.

2. **Exploratory Data Analysis (EDA):**  
   Data scientists and analysts use these tools to explore datasets, find trends, and identify anomalies before running more in-depth analyses.

3. **Operational Dashboards:**  
   Operations teams use real-time dashboards to monitor system performance, production metrics, or customer interactions.

### Examples of Open-Source Analytics and Visualization Tools:
- **Apache Superset:**  
  A modern data exploration and visualization platform with support for interactive dashboards and SQL-based data querying.

- **Metabase:**  
  An easy, open-source tool for setting up interactive dashboards with minimal coding.

- **Redash:**  
  A lightweight, open-source platform designed for querying data and building visual dashboards.


# Reflection

1. **What I Liked About This Project**  
   I enjoyed being exposed to a wide range of data engineering tools and terminology. The hands-on aspect of the project, particularly using ChatGPT to help automate the information gathering process, made it engaging. It was fascinating to see just how extensive the field of data engineering is.

2. **What I Found Hardest About This Project**  
   The most challenging part was the reading. The article was quite lengthy and seemed to cater more to experts in the field, which made it a bit daunting as a beginner. Navigating through the dense content and unfamiliar jargon required extra effort.

3. **What Surprised Me Most About This Assignment**  
   I was pleasantly surprised by how simple Quarto was to use for basic tasks. While I haven’t yet explored its full potential, I look forward to discovering its more powerful features. Another surprise was how little I understood from the article, but I’m optimistic that my comprehension will improve as the semester progresses.

4. **How I Would Approach This Project Differently Next Time**  
   Next time, I would start earlier to give myself more time to digest the content. This would allow me to engage with the material more deeply and avoid feeling rushed.
